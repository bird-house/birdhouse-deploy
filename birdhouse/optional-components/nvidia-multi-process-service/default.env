export NVIDIA_MULTIPROCESS_SERVICE_DOCKER=debian
export NVIDIA_MULTIPROCESS_SERVICE_VERSION=bookworm-slim
export NVIDIA_MULTIPROCESS_SERVICE_IMAGE='${NVIDIA_MULTIPROCESS_SERVICE_DOCKER}:${NVIDIA_MULTIPROCESS_SERVICE_VERSION}'

export DELAYED_EVAL="
  $DELAYED_EVAL
  NVIDIA_MULTIPROCESS_SERVICE_IMAGE
"
 
export JUPYTERHUB_CONFIG_OVERRIDE_INTERNAL="
${JUPYTERHUB_CONFIG_OVERRIDE_INTERNAL}

def _gpu_device_mem_limit(spawner, value):
    '''
    Set memory limits for GPUs allocated to this user.
    
    See: https://docs.nvidia.com/deploy/mps/appendix-tools-and-interface-reference.html#cuda-mps-pinned-device-mem-limit
    '''
    spawner.environment['CUDA_MPS_PINNED_DEVICE_MEM_LIMIT'] = value

def _gpu_active_thread_percentage(spawner, value):
    '''
    Set active thread percentage for GPUs allocated to this user
    
    See: https://docs.nvidia.com/deploy/mps/appendix-tools-and-interface-reference.html#cuda-mps-active-thread-percentage
    '''
    spawner.environment['CUDA_MPS_ACTIVE_THREAD_PERCENTAGE'] = str(value)

c.CustomDockerSpawner.resource_limit_callbacks.update({
  'gpu_device_mem_limit': _gpu_device_mem_limit,
  'gpu_active_thread_percentage': _gpu_active_thread_percentage,
})

def _gpu_set_mps_configs(spawner):
    '''
    Set configurations so this container uses the multi-process service running in the container named mps

    See: https://gitlab.com/nvidia/container-images/samples/-/blob/master/mps/docker-compose.yml
    '''
    spawner.extra_host_config['ipc_mode'] = 'container:mps'
    spawner.volumes['nvidia_mps'] = '/tmp/nvidia-mps'

c.CustomDockerSpawner.pre_spawn_hooks.append(_gpu_set_mps_configs)

c.CustomDockerSpawner.volumes.update({
  os.environ['NVIDIA_MPS_PROFILE_SCRIPT']: '/etc/profile.d/02-readonly-cuda-vars.sh'
})
"
